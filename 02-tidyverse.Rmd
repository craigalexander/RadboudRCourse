# Efficient Data Management in R


```{r include=F}
library(magrittr)
library(tidyverse)
```
## Tidyverse

[Tidyverse](https://www.tidyverse.org/) is a collection of R packages designed to help data scientists to make more efficient use of R. It contains the following packages (and several more, which we will:

- [tibble](http://tibble.tidyverse.org/) provides a "modern reimagining" of the standard `data.frame`, in R. Tibbles (or `tbl_df`s) are more flexible in terms of what they can store, but (purposefully) less flexible in terms of "sloppy code".
- [readr](http://readr.tidyverse.org/) provides alternative functions for reading in text data in tabular form. It provides faster and more consistent alternatives to `read.table` and `read.csv`.
- [dplyr](http://dplyr.tidyverse.org/) provides a powerful suite of functions for data manipulation with a focus on allowing for clean and simple code. We will look at `dplyr` in more detail this week.
- [ggplot2](http://ggplot2.tidyverse.org/) is a very featureful and systematic set of plotting functions, which we will focus on in this tutorial.
- [lubridate](https://lubridate.tidyverse.org/) is a very useful package for handling dates and times in R. Dates and times are often tricky to deal with, and `lubridate` provides many useful functions for efficiently handling these.

## Pipelines

Pipelines are at the centre of all the tidyverse packages. The R package [magrittr](https://cran.r-project.org/web/packages/magrittr/) provides a forward-pipe operator for R. 

Suppose we have a function `f` defined in R
```{r}
f <- function(x)
  x^2
```
Then we can apply `f` to an argument `x` using
```{r}
x <- 3
f(x)
```
The forward-pipes from magrittr allow us to rewrite this function call as 
```{r}
x %>% f
```
instead. The advantage of this alternative notation might not become immediately clear, but its advantage becomes more obvious when looking at nested function calls.

Consider the R data set `mtcars`, which contains data from the 1974 edition from the US magazine Motor Trend. Suppose we want to convert the fuel consumption to litres per 100 kilometres and then only retain the cars with a fuel economy better than 10 litres per 100 kilometres. 

```{r}
mtcars2 <- transform(mtcars, lp100k=235.21/mpg)
subset(mtcars2, lp100k<=10)
```
(If you are wondering where the number of 235.21 comes from: A US gallon is roughly 3.785 litres and a mile is roughly 1.609 kilometres, and $\frac{100\times 3.785}{1.609}\approx 235.21$)

If we want to perform both steps in one go, we can nest the two calls within one another and use
```{r,eval=FALSE}
subset(transform(mtcars, lp100k=235.21/mpg), lp100k<=10)
```
This gives exactly the same results, but is not very easy to read and understand. It is not easy to see that the argument `lp100k<=10` belongs to `subset`. When nesting function calls, the arguments get increasingly far from the function call to which they belong. 

The `%>%` operator however allows us to write this much more cleanly:
```{r,eval=FALSE}
mtcars %>%
  transform(lp100k=235.21/mpg) %>%
  subset(lp100k<=10)
```


### Task
The R code below uses pipelines. Convert it to nested function calls.
```{r, eval=FALSE}
rnorm(1000) %>% sin() %>% max()
```

### Solution
The code generates a random sample of size 1000 (from a standard normal distribution), computes the sine of each entry and then takes the maximum.
```{r}
max(sin(rnorm(1000)))
```
In this case the nested function call is easy to read because every function only takes one argument. 

### Task
Convert the R code below to pipelines.
```{r,eval=FALSE}
library(MASS)                       # Load package MASS, which contains the data
subset(transform(mammals, ratio=brain/body), ratio==max(ratio))
```
### Answer
You can use the following R code using pipelines.
```{r}
library(MASS)
mammals %>%
  transform(ratio=brain/body) %>%
  subset(ratio==max(ratio))
```
Oddly enough, ground squirrels have a higher brain-to-body weight ratio than humans.


### Additional Resources
[Pipelines for Data Analysis in R]("https://speakerdeck.com/hadley/pipelines-for-data-analysis-in-r")
Hadley Wickham has produced a series of excellent slides about pipelines, which covers much of what we will look at in this tutorial.  

[Background reading: Chapter 18 of R for Data Science]("http://r4ds.had.co.nz/pipes.html")
Chapter 18 of *R for Data Science* gives a detailed overview of pipes and some of the underpinning technology (though the latter is rather advanced).


## Tibbles

The package [tibble](https://cran.r-project.org/web/packages/tibble/) provides `tbl_df`'s (or "tibbles", which is easier to pronounce). They are a modern take on the built-in class `data.frame`.

One key advantage of tibbles is that they can store anything. A `data.frame` can only store a single value per "cell", for example a number or a character string. However, in a tibble, you can store a list or even another tibble in a cell. An example of this is the tibble `starwars` from the package `dplyr`. The column `starships` contains for each row the list of starships flown by that character (which is a list of different length depending on the character.)
```{r}
library(dplyr)                             # Load library dplyr which contains the data
starwars[,c("name", "starships")]          # Print columns name and starships
starwars[10,"starships"][[1]]              # Starships flown by Obi-Wan
```
We could not have stored this information in a data frame. We would have had to either store the information across several data frames or stored the list of starships as a character string.

### Creating tibbles

We can create tibbles using the function `tibble`. We can create the tibble from above using
```{r}
kidstibble <- tibble(name=c("Sarah", "John"), age=c(4,11), weight=c(15,28),
                     height=c(101,132), gender=c("f", "m"))
```
In other words, the function `tibble` assembles a tibble on a column-by-column basis (akin to using `cbind`). 

The function `tribble` ("transposed tibble") lets you create a tibble on a row-by-bow basis (akin to using `rbind`), which is typically more legible when creating a matrix in code.
```{r}
kidstibble <- tribble(~name,   ~age, ~weight, ~height, ~gender,
                      "Sarah",    4,      15,     101,     "f",
                      "John",    11,      28,     132,     "m")
```


### Working with tibbles

- Variables/Columns can be accessed and added using `tibble$varname` (`varname` needs to be fully spelled out). You can also access a column using `tibble[,"varname"]` or `tibble[["varname"]]`.
- Rows can be selected using `tibble[rowindices,]` (note that you cannot use row names).
- Individual cells can be accessed using `tibble[rowindices, colindices]`.

### Subsetting tibbles always results in a tibble
Tibbles are also more consistent. Subsetting tibbles always results in a tibble.
```{r}
kidstibble[,1]                            # Result is a tibble
```
In contrast, subsetting a data frame or matrix is not guaranteed to result in a data frame or matrix (unless you use `drop=FALSE`). If the result is a single column or row, subsetting a data frame or matrix results in a vector.

This "dropping" of the dimension can be very useful when using R interactively, but can be the source of many issues in more complex projects, when programmers incorrectly assume that subsetting a data frame or matrix will always result in another data frame or matrix, rather than possibly just a vector (it is thus a good idea to always use `drop=FALSE` when working with data frames or matrices in complex projects).

[Data Import Cheat Sheet]("https://github.com/rstudio/cheatsheets/raw/main/data-import.pdf")
RStudio's cheat sheet for data import also covers tibbles.

## Reading in data using readr

The package [readr](https://cran.r-project.org/web/packages/readr/index.html) contains alternatives to the functions `read.table` and `read.csv`. The alternative functions from `readr` have four main advantages.

- They read in the data a lot faster and can show a progress bar (though this is only relevant for really big data sets).
- They store the data straight in a tibble, rather than a data frame.
- They allow specifying the intended data type for each column and thus make it easier to identify rows which cause problems.
- They are less intrusive: they don't automatically convert character strings to factors and do not change column names (`read.table` and `read.csv` will for example remove spaces from variable names and replace them by full stops). The functions from `readr` are also guaranteed to give the same result irrespective of the platform or operating system they are run under.

`readr` provides the following functions.

- `read_csv` reads in comma-separated files. `read_csv2` reads in files which are semicolon-separated (common in countries like France or Germany, where a comma is used as decimal separator).
- `read_tsv` reads in tab-separated files.
- `read_delim` is the most general function (like `read.table`). The delimiter has to be specified using the argument `delim`.
- `read_fwd` reads in fixed-width files. 

All functions assume that the first row contains the column/variable names. If this is not the case, set the optional argument `col_names` to `FALSE` or to a character vector containing the intended column names.

The strings used to encode missing values can be specified using the optional argument `na`.

For example, we can read in the file [chol.txt](https://raw.githubusercontent.com/UofGAnalyticsData/R/main/Week%203/chol.txt)  using 
```{r}
library(readr)
read_delim("chol.txt", delim=" ", col_names=c("ldl", "hdl", "trig",
                                              "age", "gender", "smoke"))
```
Note that functions from `readr` show the data type it has used for each column. This makes it easier to spot mistakes like missing values not coded as expected, in which case a numeric column would show up as a character string.

For example, we can read in the file [chol.csv](https://github.com/UofGAnalyticsData/R/raw/main/Week%203/chol.csv)  using 
```{r}
library(readr)
read_csv("chol.csv", na=".")
```

### Task
Read the data files [cars.csv](https://github.com/UofGAnalyticsData/R/raw/main/Week%203/cars.csv) and [ships.txt](https://github.com/UofGAnalyticsData/R/raw/main/Week%203/ships.txt)  into R using the functions from `readr`.


### Answer
The first line of the file `cars.csv` contains the variable names and the fields are separated by commas. Missing values are encoded as asterisks.
```{r}
cars <- read_csv("cars.csv", na="*")
cars
```
We could have also used the function `read_delim`.
```{r}
read_delim("cars.csv", delim=",", na="*")
```
The first line of the file `ships.txt` contains the variable names and the fields are separated by whitespace. Missing values are encoded as ".".
```{r}
ships <- read_delim("ships.txt", delim=' ' , na=".")
ships
```


### Specifying column types
The functions from `readr` allow specifying the expected column types. This is especially important when writing which will then be run automatically. It provides an easy way of ensuring that the data provided is of the expected format.

The easiest way of specifying expected column types is to provide a character string with each letters standing for a column

| Letter  | Meaning   |
|--|------------------|
|`c`| character       |
|`i`| integer         |
|`n`| number          |
|`d`| double          |
|`l`| logical         |
|`D`| date            |
|`T`| date time       |
|`t`| time            |
|`?`| guess the type  |
|`_` or `-`| skip the column |

So for the data file chol.csv we would expect the first four columns to be integers and the latter two to be character strings, so we would use
```{r}
chol <- read_csv("chol.csv", na=".", col_types="iiiicc")
```

Specifying the expected column types can help pinpointing problems when reading in data. Suppose we had forgotten that missing values are coded using "`.`" in this data file. If we use ...
```{r}
chol <- read_csv("chol.csv")
```
... we  can see from the output that `trig` was read in as a character string, but we do not know why.

However, if we use ...
```{r}
chol <- read_csv("chol.csv", col_types="iiiicc")
```
... we obtain a warning and can print the problematic rows using
```{r}
problems(chol)
```
The output from `problems` shows us that for three rows (3, 7 and 9) the data in `chol.csv` was not of the expected format: a value of `.` is not compatible with the column being numeric. This makes it easy to identify the cause of the problem (NAs coded as "`.`") and rectify the issue.



### Additional Resources

[Data Import Cheat Sheet]("https://github.com/rstudio/cheatsheets/raw/main/data-import.pdf")
RStudio's cheat sheet for data import also covers `readr`.


[Background reading: Chapter 11 of R for Data Science]("http://r4ds.had.co.nz/data-import.html")
Chapter 11 of *R for Data Science* gives a detailed overview of the functions in `readr`. It explains in some more detail how the functions in `readr` parse files. over pipes and also covers the functions from `readr` that dead with writing files.

## Efficient data manipulation using dplyr


In this section we will work with data from Paris' Vélib' bicycle sharing system available through [JCDecaux's API](https://developer.jcdecaux.com/) for [open cycle data](https://developer.jcdecaux.com/#/opendata/).

The data consists of the number of bikes available and the number of  bike stands available at every Vélib' station, recorded every five minutes over six hours on a Tuesday afternoon in October 2017.

The data consists of two tibbles. The first, `bikes` contains data on the number of available bikes and stands at each station.

|Variable    |Description         |
|------------|--------------------|
|`name`      |Name of the station |
|`available_bikes`|Number of available at that time|
|`available_bike_stands`|Number of available bike stands|
|`time`  |Decimal time for which the number have been recorded|

The second, `stations` contains additional information about each station.

|Variable    |Description         |
|------------|--------------------|
|`name`      |Unique name of the station |
|`id`        |Internal ID number of the station |
|`address`   |Address of where the station is located|
|`lng`       |GPS coordinate (longitude) |
|`lat`       |GPS coordinate (latitude)  |
|`departement`|Département in which the station is located|

You can load the data into R using
```{r}
library(tibble)
load(url("https://github.com/UofGAnalyticsData/R/raw/main/Week%204/velib"))
```

### Overview: the key functions ("verbs") for `dplyr`

| Function ("verb") | Description | R base equivalent(s) |
|---------|-----------------------|----------------|
|`filter` |Select observations/rows|`subset`| 
|`slice`|Select observations by row numbers|`[idx,]`|
|`select` |Select variables/column |`$` or `[,sel]`| | 
|`mutate` |Create new variables/column|`transform` | 
|`arrange`|Sort observations/rows | `order`  |
|`group_by`|Group observations by variable |`by` or `aggregate`|
|`summarise`|Calculate summary statistics |`by` or `aggregate`|

The functions in `dplyr` are designed to be used with tibbles, but they also work with data frames. When invoked with a data frame, they will return a data frame as long as this is possible.

### Selecting observations (rows) using `filter` and `slice`
#### `filter`
The function `filter` is used to select observations (or rows) in a similar way to the base R function `subset`.

Suppose we want to print all bike stations in Paris (rather than other départements from Île de France)
```{r}
library(dplyr)
stations75 <- stations %>%
                filter(departement=="Paris")
stations75
```
Note the use of a double `==` to test whether the département is equal to "Paris".

We can create more complex expressions using the standard logical operators `&` ("and"), `|` ("or") and `!` ("not"). Note that you *cannot* use `&&` and `||` in this context, as they only work with scalar arguments.

For example, if we want to extract the stations which are in Paris or Hauts-de-Seine we can use
```{r}
stations7592 <- stations %>%
                  filter(departement=="Paris" | departement=="Hauts-de-Seine")
```
Rather than using a logical or we could have used `%in%`:
```{r}
stations7592 <- stations %>%
                  filter(departement %in% c("Paris" , "Hauts-de-Seine"))
```

Even though the functions from `dplyr` are designed to be used with pipelines, you can also provide the data set as first argument:
```{r, eval=FALSE}
stations7592 <- filter(stations, departement %in% c("Paris" , "Hauts-de-Seine"))
```

#### `slice`
You can use the function `slice` to select observations based on their row numbers.
```{r}
stations %>%
  slice(5:7)
```
selects the observations in rows 5 to 7 and is equivalent to
```{r}
stations[5:7,]
```

### Task
Identify the stations which had more than 60 bikes available at 3pm (i.e. `time` taking the value 15). 

### Answer
You can use the following R code:
```{r}
bikes %>%
  filter(time==15 & available_bikes>60)
```


### Selecting variables (columns) using `select`
The function `select` can be used to subset the variables (columns) of a data set.

You can either specify the columns to retain or (with a minus) those you do not want to retain.

We can only retain the name and département of each station using either
```{r}
stations.small <- stations %>%
                     select(name, departement)
stations.small
```
or
```{r}
stations.small <- stations %>% select(-id, -address, -lng, -lat)
```
You can also use `select` to change the order of the columns of a data set.

### Adding new variables using `mutate`
The function `mutate` can be used to create new variables (columns) in a data set. `mutate` is similar in functionality to the base R function `transform`.

We can add the total number of stands to the data set `bikes` using
```{r}
bikes <- bikes %>%
           mutate(total_stands = available_bikes+available_bike_stands)
```

More than one new variable can be defined by adding further arguments to `mutate`.

`transmute` is a sibling of `mutate`. Just like `mutate` it creates new columns. It however also removes all existing columns so that only the new columns remain.

### Task
The time is currently encoded as decimal (e.g. `13.5` for 13:30). Create two columns `time_hours`, which contains the hour (13 in our example), and `time_minutes`, which contains the minutes, (30 in our example).

You can calculate `time_hours` as the floor of `time` (R function `floor`) and `time_minutes` as the remainder after integer division of 60 times `time` by 60 (R operator `%%`).

### Answer
We can create both columns in one call to `mutate`.
```{r}
bikes %>%
  mutate(time_hour=floor(time), time_minutes=(60*time)%%60)
```
The output does not show the new columns (as they would take the output of a single row to more than one line). We can show them all, for example, if we remove the station name.
```{r}
bikes %>%
  mutate(time_hour=floor(time), time_minutes=(60*time)%%60) %>%
  select(-name)
```
Alternatively, we can explicitly invoke the print method of the tibble and ask it to print everything.
```{r}
bikes %>%
  mutate(time_hour=floor(time), time_minutes=(60*time)%%60) %>%
  print(width=Inf)
```


### Sorting data sets using `arrange`
The function `arrange` can be used to sort a data set by one or more variables.
We can sort the data set `bikes` by the number of available bikes suing
```{r}
bikes %>%
  arrange(available_bikes)
```
You can use the function `desc` to sort in descending order
```{r}
bikes %>%
  arrange(desc(available_bikes))
```

### Task
Identify the three bike stations that are furthest to the West (i.e. the ones with the smallest longitude `lng`).

### Answer
We first sort the stations by the longitude and the select to top three observations.
```{r}
stations %>%
  arrange(lng) %>%
  slice(1:3)
```

We could have also used the function `filter` and the ranking function `min_rank`:
```{r}
stations %>%
  filter(min_rank(lng)<=3)
```
`min_rank` returns the rank of the observation when considering the variable given as argument (there are many different ways of computing ranks, see `?min_rank` for details.)

However, the latter answer does not show the stations in increasing order of longitude. 


### Grouping data and calculating group-wise summary statistics: `group_by` and `summarise`

Suppose we want to identify the busiest stations in the system in the sense of having, on average, the most bikes taken out (and thus the highest number of available bike stands -- this is assuming JCDecaux replenish all bike stations in the same way, which is not quite what is happening in reality; there are better, but more complex, ways of defining "busy").

To calculate the average number of available bike stands per station we need to first group the data by bike station and then compute the average number of bike stands available
```{r}
bikes %>% group_by(name) %>%                             # Group by station name
  summarise(avg_stands=mean(available_bike_stands)) %>%  # Calculate averages
  arrange(desc(avg_stands))                              # Sort in descending order
```


### Task
Find the number of bike stations in each département.

You might find the function `n()` helpful, which returns the number of cases and is the `dplyr` equivalent of `COUNT(*)` in SQL (type `?n` to get help).

### Answer
We can use the following R code:
```{r}
stations %>% group_by(departement) %>%         # Group by department
  summarise(n_stations=n()) %>%                # Count cases
  arrange(desc(n_stations))                    # Sort in descending order
```



`group_by` can be also used to limit the scope of subsequent calls to other functions such as `filter`, `arrange` or `slice`. To make this more concrete, suppose we want to find for each time point the station which the most available bikes. We first have group the data by `time` and then find the station with the most available bikes.
```{r}
bikes %>%                                     
  group_by(time) %>%                           # Group by time
  arrange(desc(available_bikes)) %>%           # Sort by bikes within each group
  slice (1)                                    # Return only top one per group

```
Alternatively, we can use `filter` and `min_rank`:
```{r}
bikes %>%                                     
  group_by(time) %>%                           # Group by time
  filter(min_rank(desc(available_bikes))==1)   # Find largest in each group
```
You might have noticed that the answers differ a little. The reason for this are ties: for example, at 1.15pm the stations at Mussée d'Orsay, Mouffetard Epée de Bois and Sainte Placide Cherche-Midi all had 62 bikes available. The former commands extracts just one of them, whereas the bottom command extracts all three. 
(You would obtain the same results if you replaced `min_rank` by `row_number`, which breaks ties by using in doubt the order in the data set).

### Merging (joining) data sets using the `join`-type functions

Suppose we want to extract the data from `bikes` relating to bike stations in Hauts-de-Seine only. The table `bikes` does not however contain any information about the département in which the stations are located. We need to merge the information from the `stations` and `bikes`. This can be done using one of the `join` functions of `dplyr`. We will use `inner_join`, which only retains cases if there are corresponding entries in both data sets: this corresponds to the default behaviour of the R function `merge`.

The `join` functions will be default use the columns with common names across the two data sets ("natural join"). 

```{r}
bikes %>% inner_join(stations) %>%              # Merge data (using common variable: name)
  filter(departement=="Hauts-de-Seine")
```
We could have specified the column to used to join the data sets manually by adding the argument `by="name"` (or `by=c("name"="name")`, which allows using columns with different names in the two data set).


As a side note, in this example, we could have avoided joining the two tables. We could have first extracted the names of the stations in Hauts-de-Seine and then used those to subset the data from `bikes` (essentially the equivalent of a subquery in SQL):
```{r}
names92 <- stations %>% filter(departement=="Hauts-de-Seine") %>%
               select(name) 
bikes %>% filter(name %in% names92[[1]]) 
```
We had to use `names92[[1]]` to extract the entries of the tibble `names92` as a  character vector (we could have also used `unlist(names92)`).

You might notice a small difference in the results returned by the two approaches. The former retains the columns from `stations` which we have inserted, whereas the latter only contains the columns which `bikes` contained to start with.



### Additional Resources
[Data Transformation Cheat Sheet]("https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf")
RStudio have put together a very handy and compact cheat sheet for dplyr. 

[Background reading: Chapter 13 of R for Data Science]("http://r4ds.had.co.nz/relational-data.html")
Chapter 13 of *R for Data Science* gives a detailed overview of the functions in `dplyr`.
